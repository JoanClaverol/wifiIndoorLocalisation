{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def postResample_class(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    Given a vector with true results and the predictions of the model, \n",
    "    returns the confusion matrix, accuracy, kappa and a report(recall and recap) as a list. \n",
    "    \"\"\"    \n",
    "    # check the metrics with a confusion matrix\n",
    "    confusion_matrix = pd.crosstab(y_true, y_preds, rownames=['Real'], colnames=['Pred'])\n",
    "    print(confusion_matrix)\n",
    "    print('')\n",
    "\n",
    "    # print the accuracy\n",
    "    accuracy = sum(1 for x,y in zip(y_preds, y_true) if x == y) / len(y_true)\n",
    "    print(\"The accuracy of that model is: \", round(accuracy,4))\n",
    "\n",
    "    # kappa \n",
    "    kappa = cohen_kappa_score(y1 = y_true, y2 = y_preds)\n",
    "    print('The kappa of that model is: ', round(kappa,4))\n",
    "    print('')\n",
    "\n",
    "    # recall and recap\n",
    "    report = classification_report(y_true=y_true, y_pred=y_preds) \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../../data/raw/UJIndoorLoc/trainingData.csv')\n",
    "valid = pd.read_csv('../../data/raw/UJIndoorLoc/validationData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "wap_names = data.iloc[:,:520].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_check(temp_data, temp_valid, predict):\n",
    "\n",
    "    # train and test\n",
    "    train, test = train_test_split(temp_data, train_size=0.80, random_state=42)\n",
    "\n",
    "    logReg = LogisticRegression()\n",
    "    logReg.fit(train[wap_names], train[predict])\n",
    "\n",
    "    # check the results\n",
    "    results = postResample_class(\n",
    "        y_preds=logReg.predict(test[wap_names]), y_true=test[predict]\n",
    "    )\n",
    "    # check the results\n",
    "    results = postResample_class(\n",
    "        y_preds=logReg.predict(temp_valid[wap_names]), y_true=temp_valid[predict]\n",
    "    )\n",
    "    \n",
    "    return logReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No pre process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred     0     1     2\n",
      "Real                  \n",
      "0     1078     0     0\n",
      "1        0  1001     0\n",
      "2        0     8  1901\n",
      "\n",
      "The accuracy of that model is:  0.998\n",
      "The kappa of that model is:  0.9968\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1078\n",
      "           1       0.99      1.00      1.00      1001\n",
      "           2       1.00      1.00      1.00      1909\n",
      "\n",
      "    accuracy                           1.00      3988\n",
      "   macro avg       1.00      1.00      1.00      3988\n",
      "weighted avg       1.00      1.00      1.00      3988\n",
      "\n",
      "Pred    0    1    2\n",
      "Real               \n",
      "0     536    0    0\n",
      "1       0  306    1\n",
      "2       0    0  268\n",
      "\n",
      "The accuracy of that model is:  0.9991\n",
      "The kappa of that model is:  0.9986\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       1.00      1.00      1.00       307\n",
      "           2       1.00      1.00      1.00       268\n",
      "\n",
      "    accuracy                           1.00      1111\n",
      "   macro avg       1.00      1.00      1.00      1111\n",
      "weighted avg       1.00      1.00      1.00      1111\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joan/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train and test\n",
    "train, test = train_test_split(data, train_size=0.80, random_state=42)\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(train[wap_names], train['BUILDINGID'])\n",
    "\n",
    "# check the results\n",
    "results = postResample_class(\n",
    "    y_preds=logReg.predict(test[wap_names]), y_true=test['BUILDINGID']\n",
    ")\n",
    "# check the results\n",
    "results = postResample_class(\n",
    "    y_preds=logReg.predict(valid[wap_names]), y_true=valid['BUILDINGID']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbm value changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[wap_names] = data[wap_names].replace(to_replace=100, value=-105)\n",
    "valid[wap_names] = valid[wap_names].replace(to_replace=100, value=-105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "model_check() missing 1 required positional argument: 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-fab4f565407c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: model_check() missing 1 required positional argument: 'predict'"
     ]
    }
   ],
   "source": [
    "model_check(temp_data=data, temp_valid=valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min and max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = data.copy()\n",
    "norm_valid = valid.copy()\n",
    "\n",
    "norm_df[wap_names] = norm_df[wap_names].apply(lambda x: (x - x.min())/(x.max() - x.min()), axis='columns', result_type='expand')\n",
    "norm_valid[wap_names] = norm_valid[wap_names].apply(lambda x: (x - x.min())/(x.max() - x.min()), axis='columns', result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing missing values, by finding out the missing values\n",
    "norm_df['id'] = norm_df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df_clean = norm_df.dropna()\n",
    "norm_valid_clean = norm_valid.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(norm_valid) - len(norm_valid_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(norm_df) - len(norm_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_check(temp_data=norm_df_clean, temp_valid=norm_valid_clean, predict='BUILDINGID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_check(temp_data=norm_df_clean, temp_valid=norm_valid_clean, predict='FLOOR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicated values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupl_df = norm_df_clean.copy()\n",
    "dupl_val = norm_valid_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select observations based on unique values for LATITUDE and LONGITUDE\n",
    "dupl_df = dupl_df.drop_duplicates(['LATITUDE', 'LONGITUDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_check(temp_data=dupl_df, temp_valid=dupl_val, predict='BUILDINGID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_check(temp_data=dupl_df, temp_valid=dupl_val, predict='FLOOR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge building and floor to create a new feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df_clean['BUILDINGID_FLOOR'] = norm_df_clean['BUILDINGID'].astype(str) + \"_\" + norm_df_clean['FLOOR'].astype(str)\n",
    "norm_valid_clean['BUILDINGID_FLOOR'] = norm_valid_clean['BUILDINGID'].astype(str) + \"_\" + norm_valid_clean['FLOOR'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_check(temp_data=norm_df_clean, temp_valid=norm_valid_clean, predict='BUILDINGID_FLOOR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupl_df['BUILDINGID_FLOOR'] = dupl_df['BUILDINGID'].astype(str) + \"_\" + dupl_df['FLOOR'].astype(str)\n",
    "dupl_val['BUILDINGID_FLOOR'] = dupl_val['BUILDINGID'].astype(str) + \"_\" + dupl_val['FLOOR'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred  0_0  0_1  0_2  0_3  1_0  1_1  1_2  1_3  2_0  2_1  2_2  2_3  2_4\n",
      "Real                                                                 \n",
      "0_0     5    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "0_1     1    8    0    0    0    0    0    0    0    0    0    0    0\n",
      "0_2     0    1    5    0    0    0    0    0    0    0    0    0    0\n",
      "0_3     0    0    0    4    0    0    0    0    0    0    0    0    0\n",
      "1_0     0    0    0    0   17    1    0    0    0    0    0    0    0\n",
      "1_1     0    0    0    0    0   10    0    0    0    0    0    0    0\n",
      "1_2     0    0    0    0    0    0   14    0    0    0    0    0    0\n",
      "1_3     0    0    0    0    0    0    0    9    0    0    0    0    0\n",
      "2_0     0    0    0    0    0    0    0    0   13    0    0    0    0\n",
      "2_1     0    0    0    0    0    0    0    0    0    9    0    0    0\n",
      "2_2     0    0    0    0    0    0    0    0    0    0    8    0    0\n",
      "2_3     0    0    0    0    0    0    0    0    0    0    0   29    0\n",
      "2_4     0    0    0    0    0    0    0    0    0    0    0    2    3\n",
      "\n",
      "The accuracy of that model is:  0.964\n",
      "The kappa of that model is:  0.9597\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_0       0.83      1.00      0.91         5\n",
      "         0_1       0.89      0.89      0.89         9\n",
      "         0_2       1.00      0.83      0.91         6\n",
      "         0_3       1.00      1.00      1.00         4\n",
      "         1_0       1.00      0.94      0.97        18\n",
      "         1_1       0.91      1.00      0.95        10\n",
      "         1_2       1.00      1.00      1.00        14\n",
      "         1_3       1.00      1.00      1.00         9\n",
      "         2_0       1.00      1.00      1.00        13\n",
      "         2_1       1.00      1.00      1.00         9\n",
      "         2_2       1.00      1.00      1.00         8\n",
      "         2_3       0.94      1.00      0.97        29\n",
      "         2_4       1.00      0.60      0.75         5\n",
      "\n",
      "    accuracy                           0.96       139\n",
      "   macro avg       0.97      0.94      0.95       139\n",
      "weighted avg       0.97      0.96      0.96       139\n",
      "\n",
      "Pred  0_0  0_1  0_2  0_3  1_0  1_1  1_2  1_3  2_0  2_1  2_2  2_3  2_4\n",
      "Real                                                                 \n",
      "0_0    74    4    0    0    0    0    0    0    0    0    0    0    0\n",
      "0_1     3  204    1    0    0    0    0    0    0    0    0    0    0\n",
      "0_2     2   20  143    0    0    0    0    0    0    0    0    0    0\n",
      "0_3     0    0    5   80    0    0    0    0    0    0    0    0    0\n",
      "1_0     0    0    0    0   26    4    0    0    0    0    0    0    0\n",
      "1_1     0    0    0    0   16  114   13    0    0    0    0    0    0\n",
      "1_2     0    0    0    0    0    2   84    1    0    0    0    0    0\n",
      "1_3     0    0    0    0    0    0    6   41    0    0    0    0    0\n",
      "2_0     0    0    0    0    0    0    0    0   22    2    0    0    0\n",
      "2_1     0    0    0    0    0    0    0    0    0  109    2    0    0\n",
      "2_2     0    0    0    0    0    0    0    0    0    1   48    4    1\n",
      "2_3     0    0    0    0    0    0    0    0    0    0    0   40    0\n",
      "2_4     0    0    0    0    1    0    0    0    0    0    0    9   29\n",
      "\n",
      "The accuracy of that model is:  0.9127\n",
      "The kappa of that model is:  0.9021\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_0       0.94      0.95      0.94        78\n",
      "         0_1       0.89      0.98      0.94       208\n",
      "         0_2       0.96      0.87      0.91       165\n",
      "         0_3       1.00      0.94      0.97        85\n",
      "         1_0       0.60      0.87      0.71        30\n",
      "         1_1       0.95      0.80      0.87       143\n",
      "         1_2       0.82      0.97      0.88        87\n",
      "         1_3       0.98      0.87      0.92        47\n",
      "         2_0       1.00      0.92      0.96        24\n",
      "         2_1       0.97      0.98      0.98       111\n",
      "         2_2       0.96      0.89      0.92        54\n",
      "         2_3       0.75      1.00      0.86        40\n",
      "         2_4       0.97      0.74      0.84        39\n",
      "\n",
      "    accuracy                           0.91      1111\n",
      "   macro avg       0.91      0.91      0.90      1111\n",
      "weighted avg       0.92      0.91      0.91      1111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model_check(temp_data=dupl_df, temp_valid=dupl_val, predict='BUILDINGID_FLOOR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(dupl_val[wap_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_check = dupl_val[['BUILDINGID','FLOOR']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_check['preds'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_col = valid_check['preds'].str.split('_', n=2, expand=True).rename(columns={0: 'preds_b', 1: 'preds_f'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_check = pd.concat([valid_check, preds_col], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred  0.0  1.0  2.0\n",
      "Real               \n",
      "0     536    0    0\n",
      "1       0  307    0\n",
      "2       0    1  267\n",
      "\n",
      "The accuracy of that model is:  0.9991\n",
      "The kappa of that model is:  0.9986\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       1.00      1.00      1.00       307\n",
      "           2       1.00      1.00      1.00       268\n",
      "\n",
      "    accuracy                           1.00      1111\n",
      "   macro avg       1.00      1.00      1.00      1111\n",
      "weighted avg       1.00      1.00      1.00      1111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "postResample_class(y_preds=valid_check['preds_b'].astype(float), y_true=valid_check['BUILDINGID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred  0.0  1.0  2.0  3.0  4.0\n",
      "Real                         \n",
      "0     122   10    0    0    0\n",
      "1      19  427   16    0    0\n",
      "2       2   23  275    5    1\n",
      "3       0    0   11  161    0\n",
      "4       1    0    0    9   29\n",
      "\n",
      "The accuracy of that model is:  0.9127\n",
      "The kappa of that model is:  0.8774\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       132\n",
      "           1       0.93      0.92      0.93       462\n",
      "           2       0.91      0.90      0.90       306\n",
      "           3       0.92      0.94      0.93       172\n",
      "           4       0.97      0.74      0.84        39\n",
      "\n",
      "    accuracy                           0.91      1111\n",
      "   macro avg       0.91      0.89      0.90      1111\n",
      "weighted avg       0.91      0.91      0.91      1111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "postResample_class(y_preds=valid_check['preds_f'].astype(float), y_true=valid_check['FLOOR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous analysis, we can use the logistic regression to predict the building and then, create a different model by building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarising the best steps and store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise all the steps done since now\n",
    "# 1. load the data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../../data/raw/UJIndoorLoc/trainingData.csv')\n",
    "valid = pd.read_csv('../../data/raw/UJIndoorLoc/validationData.csv')\n",
    "wap_names = data.iloc[:,:520].columns.tolist()\n",
    "\n",
    "# 2. transform the values from 100dBm to -105dBm\n",
    "data[wap_names] = data[wap_names].replace(to_replace=100, value=-105)\n",
    "valid[wap_names] = valid[wap_names].replace(to_replace=100, value=-105)\n",
    "\n",
    "\n",
    "# 3. Normalize the waps by row by min and max\n",
    "norm_df = data.copy()\n",
    "norm_valid = valid.copy()\n",
    "\n",
    "norm_df[wap_names] = norm_df[wap_names].apply(lambda x: (x - x.min())/(x.max() - x.min()), axis='columns', result_type='expand')\n",
    "norm_df['id'] = str(norm_df.index + 1)\n",
    "norm_df_clean = norm_df.dropna() # missing values are created in 76 rows, as they have 0 variance (further exploration)\n",
    "norm_valid[wap_names] = norm_valid[wap_names].apply(lambda x: (x - x.min())/(x.max() - x.min()), axis='columns', result_type='expand')\n",
    "\n",
    "\n",
    "# 4. Store results\n",
    "norm_df_clean.to_csv('../../data/clean/norm_training.csv')\n",
    "norm_valid.to_csv('../../data/clean/norm_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BUILDINGID_FLOOR\n",
       "0_0     54\n",
       "0_1     63\n",
       "0_2     26\n",
       "0_3     20\n",
       "1_0     75\n",
       "1_1     48\n",
       "1_2     75\n",
       "1_3     40\n",
       "2_0     58\n",
       "2_1     59\n",
       "2_2     36\n",
       "2_3    110\n",
       "2_4     28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main problems are placed in:\n",
    "#     * building 1 floor 0. Accuracy 60%\n",
    "#     * building 2 floor 3 Accuracy 75%\n",
    "dupl_df.groupby(['BUILDINGID_FLOOR']).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
